# Evaluation

This folder contains scripts and results for intermediate evaluation, mostly based on zero-shot prompting performance. For now, these are performed with Eleuther AI's [LM eval harness](https://github.com/EleutherAI/lm-evaluation-harness).

Currently evaluated models:
- [13B](https://github.com/bigscience-workshop/bigscience/blob/master/evaluation/Tr1-13B-harness-eval.json)
- ...
